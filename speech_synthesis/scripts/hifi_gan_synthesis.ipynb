{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e353aa7",
   "metadata": {},
   "source": [
    "# HIFI-GAN synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ece65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code: https://github.com/jik876/hifi-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7004f679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Programming\\\\Github\\\\hifi-gan', 'C:\\\\Programming\\\\Github\\\\waveglow\\\\tacotron2', 'C:\\\\Programming\\\\GitLab\\\\radarspeech\\\\speech_synthesis\\\\scripts', 'C:\\\\Python39\\\\python39.zip', 'C:\\\\Python39\\\\DLLs', 'C:\\\\Python39\\\\lib', 'C:\\\\Python39', '', 'C:\\\\Python39\\\\lib\\\\site-packages', 'C:\\\\Python39\\\\lib\\\\site-packages\\\\phonemizer-3.2.1-py3.9.egg', 'C:\\\\Python39\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Python39\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Python39\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import simpleaudio as sa\n",
    "import librosa\n",
    "import soundfile\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg', force=True)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "sys.path.insert(0,\"C:\\\\Programming\\\\Github\\waveglow\\\\tacotron2\")\n",
    "sys.path.insert(0,\"C:\\\\Programming\\\\Github\\\\hifi-gan\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9babc817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def play_audio(audio_data, fs_Hz):\n",
    "    ''' Function to convert a numpy array to 16 bit integer array\n",
    "        and serialize it to play it with simpleaudio.\n",
    "        Args: \n",
    "            audio_data (1d-numpy array): array holding the audio samples.\n",
    "            fs_Hz (scalar): Sampling frequency.\n",
    "\n",
    "        Returns: \n",
    "            None.\n",
    "    '''\n",
    "    audio_data_16bit = audio_data\n",
    "    nan_indices = np.argwhere(np.isnan(audio_data_16bit))\n",
    "    if(nan_indices.size > 0):\n",
    "        print(\"Error: there are %d NaN values in the waveform. Play aborted.\" %(nan_indices.size))\n",
    "        return\n",
    "\n",
    "    max_value = np.max(abs(audio_data))\n",
    "\n",
    "    audio_data_16bit = audio_data_16bit*32767/max_value\n",
    "    audio_data_16bit = audio_data_16bit.astype(np.int16)\n",
    "    play_obj = sa.play_buffer(audio_data_16bit, 1, 2, fs_Hz)\n",
    "    play_obj.wait_done()\n",
    "    \n",
    "    return audio_data_16bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188cb75",
   "metadata": {},
   "source": [
    "## Load the model and inference functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde0bc75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copied from inference_e2e.py\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from env import AttrDict\n",
    "from meldataset import MAX_WAV_VALUE\n",
    "from models import Generator\n",
    "\n",
    "h = None\n",
    "device = None\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict\n",
    "\n",
    "\n",
    "def scan_checkpoint(cp_dir, prefix):\n",
    "    pattern = os.path.join(cp_dir, prefix + '*')\n",
    "    cp_list = glob.glob(pattern)\n",
    "    if len(cp_list) == 0:\n",
    "        return ''\n",
    "    return sorted(cp_list)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919703f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_mels_dir = \"C:\\\\Users\\\\chris\\\\Documents\\\\Institut\\\\corpora\\\\\" \\\n",
    "    + \"speech_synthesis\\\\corpus_six\\\\S003\\\\input_params_files\\\\tacotron_mel_power_spectrogram\\\\npy\"\n",
    "\n",
    "time_context_frames = 50\n",
    "fs_hifigan_Hz = 22050\n",
    "\n",
    "hifi_gan_base_path = \"C:\\Programming\\Github\\hifi-gan\"\n",
    "# input_mels_dir = os.path.join(hifi_gan_base_path, \"test_mel_files\")\n",
    "# output_dir = os.path.join(hifi_gan_base_path, \"generated_files_from_mel\")\n",
    "output_dir = os.path.join(os.getcwd(), \"hifi_gan_synthesis_output\")\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    print(\"Created folder {:s}\".format(output_dir))\n",
    "                          \n",
    "checkpoint_file = os.path.join(hifi_gan_base_path, \"checkpoints\\\\UNIVERSAL_V1\\\\g_02500000\")\n",
    "\n",
    "config_file = os.path.join(hifi_gan_base_path, 'config.json')\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "torch.manual_seed(h.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(h.seed)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Instantiate the generator model.\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(checkpoint_file, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "\n",
    "filelist = os.listdir(input_mels_dir)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()\n",
    "\n",
    "# Generate all files.\n",
    "with torch.no_grad():\n",
    "    for i, filname in enumerate(filelist):\n",
    "        x = np.load(os.path.join(input_mels_dir, filname))\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        y_g_hat = generator(x)\n",
    "        audio = y_g_hat.squeeze()\n",
    "        audio = audio * MAX_WAV_VALUE\n",
    "        audio = audio.cpu().numpy().astype('int16')\n",
    "        # plt.plot(audio)\n",
    "        # plt.show()\n",
    "        stop_time = time.perf_counter()\n",
    "        inference_time = stop_time-start_time\n",
    "        \n",
    "        print(\"Mel conversion took {:1.4f} s\".format(inference_time))\n",
    "        print(\"Real-time factor for wave-glow inference: {:1.3}\".format(len(audio)/fs_hifigan_Hz/inference_time))\n",
    "\n",
    "        output_file = os.path.join(output_dir, os.path.splitext(filname)[0] + '_generated_e2e.wav')\n",
    "        audio[:160*time_context_frames] = 0 # Remove network settling time.\n",
    "        write(output_file, fs_hifigan_Hz, audio)\n",
    "        print(output_file)\n",
    "print(\"Finished generation.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc73918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chris\\\\Documents\\\\Institut\\\\corpora\\\\speech_synthesis\\\\corpus_six\\\\S004\\\\input_params_files\\\\tacotron_mel_power_spectrogram\\npy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mels_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3ddcf",
   "metadata": {},
   "source": [
    "## Test the HIFI-GAN vocoder on a frame by frame basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa60ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_mels_dir = \"C:\\\\Programming\\\\GitLab\\\\radarspeech\\\\speech_synthesis\" \\\n",
    "    + \"\\\\saved_results\\\\corpus_six\\\\crn\\\\phase_mag_db_delta_phase_delta\\\\run_21_20_44_9\\\\tacotron_mel_power_spectrogram_predicted_9_21_20_44\\\\npy\"\n",
    "fs_hifigan_Hz = 22050\n",
    "time_context_frames = 50\n",
    "\n",
    "hifi_gan_base_path = \"C:\\Programming\\Github\\hifi-gan\"\n",
    "# input_mels_dir = os.path.join(hifi_gan_base_path, \"test_mel_files\")\n",
    "# output_dir = os.path.join(hifi_gan_base_path, \"generated_files_from_mel\")\n",
    "output_dir = os.path.join(os.getcwd(), \"hifi_gan_synthesis_output\")\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    print(\"Created folder {:s}\".format(output_dir))\n",
    "                          \n",
    "checkpoint_file = os.path.join(hifi_gan_base_path, \"checkpoints\\\\UNIVERSAL_V1\\\\g_02500000\")\n",
    "\n",
    "config_file = os.path.join(hifi_gan_base_path, 'config.json')\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "torch.manual_seed(h.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(h.seed)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Instantiate the generator model.\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(checkpoint_file, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "\n",
    "filelist = os.listdir(input_mels_dir)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()\n",
    "\n",
    "# Generate all files.\n",
    "with torch.no_grad():\n",
    "    for i, filname in enumerate(filelist):\n",
    "        x = np.load(os.path.join(input_mels_dir, filname))\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Predict frame by frame.\n",
    "        frame_by_frame_audio = torch.empty((1,1,1))\n",
    "        for frame_index in range(x.shape[-1]):\n",
    "            input_mel_frame = x[0,:,frame_index].unsqueeze(dim=0).unsqueeze(dim=2)\n",
    "            audio_frame = generator(input_mel_frame)\n",
    "            frame_by_frame_audio = torch.cat((frame_by_frame_audio, audio_frame), dim=-1)\n",
    "            \n",
    "        frame_by_frame_audio = frame_by_frame_audio.squeeze().squeeze()\n",
    "        frame_by_frame_audio = frame_by_frame_audio * MAX_WAV_VALUE\n",
    "        frame_by_frame_audio = frame_by_frame_audio.cpu().numpy().astype('int16')  \n",
    "        \n",
    "        # predict in one go.\n",
    "        y_g_hat = generator(x)\n",
    "        audio = y_g_hat.squeeze()\n",
    "        audio = audio * MAX_WAV_VALUE\n",
    "        audio = audio.cpu().numpy().astype('int16')\n",
    "        \n",
    "        plt.plot(audio)\n",
    "        plt.plot(frame_by_frame_audio)\n",
    "        plt.show()\n",
    "        \n",
    "        stop_time = time.perf_counter()\n",
    "        inference_time = stop_time-start_time\n",
    "        \n",
    "        output_file = os.path.join(output_dir, os.path.splitext(filname)[0] + '_generated_e2e.wav')\n",
    "        output_file_fbf = os.path.join(output_dir, os.path.splitext(filname)[0] + '_generated_e2e_frame_by_frame.wav')\n",
    "        audio[:160*time_context_frames] = 0 # Remove network settling time.\n",
    "        write(output_file_fbf, fs_hifigan_Hz, frame_by_frame_audio)\n",
    "        write(output_file, fs_hifigan_Hz, audio)\n",
    "        break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
